<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aneesh Rangnekar</title>
  
  <meta name="author" content="Aneesh Rangnekar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:60%;border:0;border-spacing:0;margin:auto;"><tbody>
    <tr><td>

      <!-- Header -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:2.5%;width:63%;vertical-align:middle">
            <p style="text-align:center">
              <name>Aneesh Rangnekar</name>
            </p>
            <p>
              I am a Research Fellow in the Department of Medical Physics at Memorial Sloan Kettering Cancer Center, working under the mentorship of <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Dr. Harini Veeraraghavan</a>. My research focuses on applying machine learning to medical imaging to for enhancing clinical decision-making, with focus on for parameter-efficient and robust application of foundation models. I completed my PhD at <a href="https://www.rit.edu/science/chester-f-carlson-center-imaging-science">The Chester F. Carlson Center for Imaging Science at Rochester Institute of Technology (RIT)</a>, advised by <a href="https://people.rit.edu/mjhsma/">Dr. Matthew Hoffman</a> and co-advised by <a href="https://chriskanan.com/">Dr. Christopher Kanan</a> and <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Dr. Emmett Ientilucci</a>.
            </p>
            <p style="text-align:left">
              <a href="mailto:aneesh.rangnekar@gmail.com">Email</a> &nbsp;/&nbsp;
              <a href="https://github.com/aneesh3108">GitHub</a> &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?hl=en&user=2UtY2BIAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
              <a href="resume/AneeshR_CV.pdf">CV (PDF)</a>
            </p>
          </td>
          <td style="padding:2.5%;width:37%;max-width:37%">
            <a href="images/profile_pic_nanobanana.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;"
               alt="profile photo" src="images/profile_pic_nanobanana.png" class="hoverZoomLink"></a>
          </td>
        </tr>
      </tbody></table>
      
      <!-- Research -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:20px;vertical-align:middle">
            <div style="display:flex; justify-content:space-between; align-items:baseline;">
              <heading role="heading" aria-level="2">Publications</heading>
            </div>
          </td>
        </tr>
      </tbody></table>

      <!-- Paper entries -->

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/paper2025_brat.png' width="100%" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              brat: Aligned Multi-View Embeddings for Brain MRI Analysis
            </papertitle><br>
            <a href="https://www.bdi.ox.ac.uk/Team/maxime-kayser">Maxime Kayser</a>, Maksim Gridnev, Wanting Wang, Max Bain, <strong>Aneesh Rangnekar</strong>, Avijit Chatterjee, Aleksandr Petrov, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a>, <a href="https://www.mskcc.org/cancer-care/doctors/nathaniel-swinburne">Nathaniel C. Swinburne</a><br>
            <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2026<br>
            <a href="papers/brat_overleaf_copy.pdf">Paper</a>
            <p style="text-align:justify;">
              We developed <span style="background-color:lightgreen; font-style:italic;">brat</span> (Brain Report Alignment Transformer), trained on 75k MRI–report pairs with a pairwise view alignment and diversity-promoting loss. Leveraging these mechanisms, <span style="background-color:lightgreen; font-style:italic;">brat</span> outperforms prior methods on image-text retrieval, tumor segmentation, and Alzheimer’s classification, and also enables high-quality automatic report generation from MRIs using language models.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/paper2025_ood_rf.png' width="100%" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Random forest-based out-of-distribution detection for robust lung cancer segmentation
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>,  <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2508.19112">arXiv</a>
            <p style="text-align:justify;">
              Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. We proposed RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. RF-Deep achieved strong detection performance (with FPR95 < 0.1% on far-OOD cases), outperforming established OOD approaches.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2025_heartsub.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>,  Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, <a href="https://www.mskcc.org/cancer-care/doctors/abraham-wu">Abraham Wu</a>, <a href="https://www.mskcc.org/profile/maria-thor">Maria Thor</a>, Andreas Rimner, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2505.10855">arXiv</a>
            <p style="text-align:justify;">
              We fine-tuned a pretrained transformer with a convolution decoder for cardiac substructures segmentation in contrast and non-contrast CT scans, with an emphasis on reducing the need for extensive annotated data. Pretrained on large in-the-wild CT dataset, the model was adapted for radiotherapy planning in lung cancer patients, with zero-shot application on breast cancer patients. Extensive experiments demonstrated strong generalization across imaging modalities, clinical sites, and patient positioning.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

       <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/medphycs2025_sslmodels.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Self-supervised learning improves robustness of deep learning lung tumor segmentation models to CT imaging differences
            </papertitle><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em>Medical Physics</em>, 2025<br>
            <a href="https://arxiv.org/pdf/2405.08657">arXiv</a> / <a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.17541">Publication</a>
            <p style="text-align:justify;">
              We investigated the benefits of large-scale in-the-wild self-supervised pretraining on uncurated CT scans to improve robustness in tumor segmentation, with a focus on lung cancer tumors. When fine-tuned on smaller curated NSCLC datasets, Swin transformer models pretrained on this diverse unlabeled data, consistently outperformed both self-pretrained Swin and Vision Transformer counterparts across varied CT acquisition protocols. Our study directly addresses whether self-supervision on noisy, heterogeneous CT data improves generalization to real-world distribution shifts — a critical gap underaddressed by prior research.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/midl2025_codistilled.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Co-distilled attention guided masked image modeling with noisy teacher for self-supervised learning on medical images
            </papertitle><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em> Medical Imaging with Deep Learning</em>, 2025<br>
            <a href="papers/MIDL_2025__CoDistilled.pdf">Paper</a> / <a href="https://openreview.net/forum?id=tLYEAmMupF#discussion">OpenReview</a>
            <p style="text-align:justify;">
              We developed DAGMaN, a novel self-supervised learning framework that combined attention-guided masked image modeling and noisy teacher co-distillation for medical imaging. We integrated global semantic attention into Swin transformers with vision transformer based multi-head self-attention blocks, and improved feature learning and attention diversity by noise injection in the teacher’s pipeline. Our approach outperformed prior methods on multiple medical tasks and achieved high accuracy in few-shot settings, while enabling better interpretability via attention map visualization, a previously unavailable feature for Swin transformers.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/spie2025_quantuncertainty.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Quantifying uncertainty in lung cancer segmentation with foundation models applied to mixed-domain datasets
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, Nishant Nadkarni, <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em>SPIE Medical Imaging</em>, 2025<br>
            <a href="https://arxiv.org/pdf/2403.13113.pdf">arXiv</a> / <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13409/1340914/Quantifying-uncertainty-in-lung-cancer-segmentation-with-foundation-models-applied/10.1117/12.3047709.short">Publication</a>
            <p style="text-align:justify;">
              We investigated applications of foundation models on lung tumor segmentation across mixed-domain CT datasets, encompassing diverse acquisition protocols and institutions, as a stepping stone towards task generalization. Our study evaluated segmentation performance and uncertainty estimation using Monte Carlo dropout, deep ensembles, and test-time augmentation. We demonstrated that fast, entropy-based metrics and volumetric occupancy can effectively track model performance under domain shift, offering introductory practical tools to evaluate segmentation trustworthiness in mixed-domain clinical settings.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2025_smart.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Self-distilled Masked Attention guided masked image modeling with noise Regularized Teacher (SMART) for medical image analysis
            </papertitle><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, Chloe Choi, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2310.01209">arXiv</a> / <a href="presentations/AAPM 2024 - SNAP - SMART.pdf">AAPM 2024 SNAP Oral</a>
            <p style="text-align:justify;">
              We developed a new self-supervised framework for medical imaging with Swin Transformers, which traditionally lack the <span style="background-color:rgb(239, 168, 168); font-style:italic;">[CLS]</span> token used with masked image modeling. We introduced a semantic attention module for global masking and a noise-regularized momentum teacher for stable co-distillation. This yielded a stronger foundation model, improving tumor/organ segmentation and enabling zero-shot localization via <span style="background-color:rgb(239, 168, 168); font-style:italic;">[CLS]</span> global attention maps.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2024_ovarian_aiguided.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>	
              Improving ovarian cancer segmentation accuracy with transformers through AI-guided labeling
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/research-areas/labs/members/kevin-boehm">Kevin M. Boehm</a>, Emily A. Aherne, Ines Nikolovski, Natalie Gangai, Ying Liu, Dimitry Zamarin, Kara Roche, <a href="https://www.mskcc.org/research-areas/labs/sohrab-shah">Sohrab Shah</a>, <a href="https://www.mskcc.org/cancer-care/doctors/yulia-lakhman">Yulia Lakhman</a>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2406.17666">arXiv</a>
            <p style="text-align:justify;">
              We formulated an AI-guided labeling pipeline using a multi-resolution residual 2D network trained on partially segmented CTs (adnexal tumors and omental implants) to assist radiologists in refining annotations. The enhanced dataset fine-tuned two transformer architectures, namely SMIT and Swin UNETR, and is evaluated across 71 multi-institutional 3D CT scans. Training with AI-refined labels yielded statistically significant improvements across all metrics for both models. Our approach emphasized efficient dataset curation, reducing annotation workload for radiologists while improving model performance and radiomics reproducibility in ovarian cancer.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/s4al.jpg' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Semantic Segmentation with Active Semi-Supervised Learning
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2023<br>
            <a href="https://arxiv.org/pdf/2203.10730">arXiv</a> / <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Rangnekar_Semantic_Segmentation_With_Active_Semi-Supervised_Learning_WACV_2023_paper.pdf">Publication</a> / <a href="https://github.com/aneesh3108/S4AL">Code</a> / <a href="presentations/WACV 2023 - S4AL - Poster.pdf">Poster</a> / <a href="presentations/WACV 2023 - S4AL - Presentation.pdf">Presentation</a>
            <p style="text-align:justify;">
              We developed S4AL, a hybrid approach that combined semi-supervised and active learning, for data-efficient semantic segmentation. It uses a teacher–student pseudo-labeling framework to generate region-level acquisition scores, enabling querying and annotating of only the most informative regions rather than full images. We introduced two regularization techniques, confidence weighting and balanced ClassMix, that helped mitigate class imbalance and enhance quality of the acquisition metric. S4AL achieved over 95% of full-dataset performance using less than 17% of pixel annotations on CamVid and CityScapes datasets.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/s4alplus.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Semantic Segmentation with Active Semi-Supervised Representation Learning
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>British Machine Vision Conference (BMVC)</em>, 2022<br>
            <a href="https://arxiv.org/pdf/2210.08403">arXiv</a> / <a href="https://bmvc2022.mpi-inf.mpg.de/0229.pdf">Publication</a> / <a href="presentations/BMVC 2022 - S4ALv2 - Poster.pdf">Poster</a> / <a href="presentations/BMVC 2022 - S4ALv2 - Presentation.pdf">Presentation</a>
            <p style="text-align:justify;">
              We developed S4AL+, a hybrid framework that combines semi-supervised learning with active learning for semantic segmentation, aiming to reduce annotation costs. We replaced the conventional mean-teacher approach with self-training using noisy pseudo-labels and added a contrastive head for better feature learning of the classes. On benchmarks CamVid and CityScapes, S4AL+ achieved over 95% of full-label performance using just 12–15% of labeled data, outperforming state-of-the-art approaches at the time.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/cvprw2022_pbvs_dataset.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Semi-Supervised Hyperspectral Object Detection Challenge Results - PBVS 2022
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.rit.edu/dirs/students/zachary-mulhollan">Zachary Mulhollan</a>, <a href="https://www.rit.edu/directory/axvpci-anthony-vodacek">Anthony Vodacek</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a>, <a href="https://pages.cvc.uab.es/asappa/">Angel Sappa</a>, <a href="https://sites.google.com/site/erikblasch/">Erik Blasch</a>, et al.
            </a><br>
            <em>Computer Vision and Pattern Recognition (CVPR) workshops</em>, 2022<br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Rangnekar_Semi-Supervised_Hyperspectral_Object_Detection_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.pdf">Publication</a>
            <p style="text-align:justify;">
              We curated the first true hyperspectral object detection dataset, collected from a university rooftop overlooking a 4-way intersection over three days. The dataset consists of 2890 temporally contiguous frames at ~1600×192 resolution, spanning 51 spectral bands from 400–900nm. To capture real-world variability, the training, validation, and test datasets were acquired on different days under varying weather conditions. Labels are provided for both fully-supervised and semi-supervised settings, encouraging a competition hosted at CVPR Perception Beyond the Visible Spectrum workshop.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/dddas2022_specal.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              SpecAL: Towards Active Learning for Semantic Segmentation of Hyperspectral Imagery
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Emmett Ientilucci</a>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Dynamic Data Driven Applications Systems (DDDAS)</em>, 2022<br>
            <a href="papers/DDDAS_2022__SpecAL.pdf">Paper</a> / <a href="https://link.springer.com/chapter/10.1007/978-3-031-52670-1_19">Publication</a>
            <p style="text-align:justify;">
              We proposed SpecAL, an active learning framework for semantic segmentation of hyperspectral imagery, reducing the need for extensive efforts for labeled data. Using the AeroRIT dataset, we combine data-efficient neural network design with self-supervised learning and batch-ensembles based uncertainty acquisition to iteratively improve performance. Our method design achieved oracle level segmentation performance using only 30% of the labeled data, demonstrating a scalable path for annotation-efficient hyperspectral analysis.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>
      
      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/dddas2020_uncestimation.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Uncertainty estimation for semantic segmentation of hyperspectral imagery
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Emmett Ientilucci</a>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Dynamic Data Driven Applications Systems (DDDAS)</em>, 2020<br>
            <a href="papers/DDDAS_2020__Uncertainty_Estimation_in_HSI.pdf">Paper</a> / <a href="https://link.springer.com/chapter/10.1007/978-3-030-61725-7_20">Publication</a>
            <p style="text-align:justify;">
              We extended deep learning for hyperspectral imaging on the AeroRIT dataset by evaluating network uncertainty within a Dynamic Data-Driven Applications Systems (DDDAS) framework. Using Deep Ensembles, Monte Carlo Dropout, and Batch Ensembles with a modified U-Net, we studied robust pixel-level segmentation under noisy, atmosphere-sensitive signals. Our results highlighted uncertainty estimation as key to guiding resource allocation and improving hyperspectral semantic segmentation.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/tgrs2020_aerorit.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              AeroRIT: A New Scene for Hyperspectral Image Analysis
            </papertitle><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://ieeexplore.ieee.org/author/37088541291">Nilay Mokashi</a>, <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Emmett Ientilucci</a>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Transactions on Geoscience and Remote Sensing (TGRS)</em>, 2020<br>
            <a href="https://arxiv.org/pdf/1912.08178.pdf">arXiv</a> / <a href="https://ieeexplore.ieee.org/abstract/document/9079478/">Publication</a> / <a href="https://github.com/aneesh3108/AeroRIT">Code</a>
            <p style="text-align:justify;">
              We introduced AeroRIT, a new aerial hyperspectral dataset designed specifically to support convolutional neural network (CNN) training for scene understanding. Unlike typical airborne hyperspectral datasets focused on vegetation or roads, AeroRIT includes buildings and cars, expanding the domain diversity. ed and benchmarked several CNN architectures on AeroRIT, thoroughly evaluating classification accuracy, spatial consistency, and generalization across scenes. 
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/tgrs2018_deephkcf.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Tracking in aerial hyperspectral videos using deep kernelized correlation filters
            </papertitle><br>
            <a href="https://uzkent.github.io/">Burak Uzkent</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Transactions on Geoscience and Remote Sensing (TGRS)</em>, 2018<br>
            <a href="https://arxiv.org/pdf/1711.07235">arXiv</a> / <a href="https://ieeexplore.ieee.org/document/8435971">Publication</a>
            <p style="text-align:justify;">
              We developed DeepHKCF, a hyperspectral aerial vehicle tracker that combines kernelized correlation filters (KCFs) with deep CNN features, leveraging adaptive multimodal hyperspectral sensors. A single KCF-in-multiple-ROIs strategy with efficient ROI mapping addresses low temporal resolution while enabling fast feature extraction and flexibility to integrate advanced correlation filter trackers. Experiments on DIRSIG-simulated hyperspectral videos show strong tracking performance, along with a released large-scale synthetic dataset for vehicle classification in wide-area motion imagery (WAMI).
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/cvprw2017_likelihoodmaps.png' width="200" loading="lazy">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>
              Aerial vehicle tracking by adaptive fusion of hyperspectral likelihood maps
            </papertitle><br>
            <a href="https://uzkent.github.io/">Burak Uzkent</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>Computer Vision and Pattern Recognition (CVPR) workshops</em>, 2017<br>
            <a href="https://arxiv.org/pdf/1707.03553">arXiv</a> / <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/papers/Uzkent_Aerial_Vehicle_Tracking_CVPR_2017_paper.pdf">Publication</a>
            <p style="text-align:justify;">
              We developed hyperspectral likelihood maps-aided tracking (HLT), a real-time hyperspectral tracking method that learns a generative target model online without offline classifiers or heavy hyperparameter tuning. It adaptively fuses likelihood maps across visible-to-infrared bands into a distinctive representation that separates foreground from background. Experiments show HLT outperforms existing fusion methods and matches state-of-the-art hyperspectral tracking frameworks.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>


      <!-- Footer -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br><p style="text-align:center;font-size:small;">
              This website is adapted from Jon Barron's template (<a href="https://jonbarron.info/">site</a>, <a href="https://github.com/jonbarron/jonbarron_website">source code</a>)
            </p>
          </td>
        </tr>
      </tbody></table>

    </td></tr>
  </tbody></table>
</body>
</html>
