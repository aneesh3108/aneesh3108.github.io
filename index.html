<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aneesh Rangnekar</title>
  
  <meta name="author" content="Aneesh Rangnekar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0;border-spacing:0;margin:auto;"><tbody>
    <tr><td>

      <!-- Header -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:2.5%;width:63%;vertical-align:middle">
            <p style="text-align:center">
              <name>Aneesh Rangnekar</name>
            </p>
            <p>
              I am a Research Fellow in the Department of Medical Physics at Memorial Sloan Kettering Cancer Center, working under the mentorship of <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Dr. Harini Veeraraghavan</a>. My research focuses on applying machine learning to medical imaging to for enhancing clinical decision-making, with focus on for parameter-efficient and robust application of foundation models. I completed my PhD at <a href="https://www.rit.edu/science/chester-f-carlson-center-imaging-science">The Chester F. Carlson Center for Imaging Science at Rochester Institute of Technology (RIT)</a>, advised by <a href="https://people.rit.edu/mjhsma/">Dr. Matthew Hoffman</a> and co-advised by <a href="https://chriskanan.com/">Dr. Christopher Kanan</a> and <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Dr. Emmett Ientilucci</a>.
            </p>
            <p style="text-align:center">
              <a href="mailto:rangnea@mskcc.org">Email (MSKCC)</a> &nbsp/&nbsp
              <a href="mailto:aneesh.rangnekar@gmail.com">Email (Personal)</a> &nbsp/&nbsp
              <a href="https://github.com/aneesh3108">Github</a> &nbsp/&nbsp
              <a href="resume/AneeshR_CV_2page.pdf">CV</a>
            </p>
          </td>
          <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/profile_pic.png"><img style="width:100%" alt="profile photo" src="images/profile_pic.png"></a>
          </td>
        </tr>
      </tbody></table>
      <!-- Research -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:20px;vertical-align:middle">
            <div style="display:flex; justify-content:space-between; align-items:baseline;">
              <heading>Selected Publications</heading>
              <a href="https://scholar.google.com/citations?hl=en&user=2UtY2BIAAAAJ&view_op=list_works&sortby=pubdate" style="font-size:14px; text-decoration:none; color:#1772d0;">View full list →</a>
            </div>
          </td>
        </tr>
      </tbody></table>

      <!-- Paper entries -->

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/paper2025_ood_rf.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Beyond accuracy metrics: out-of-distribution for determining reliability
              of segmentation models in medical image segmentation for CT
            </papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            Under preparation.
            <p style="text-align:justify;">This work introduces a machine learning based approach for detecting out-of-distribution (OOD) cases using hierarchical transformer (Swin) features, with focus on 3D lung cancer scans. We benchmark three leading self-supervised learning (SSL) methods to demonstrate the feasibility of our performance — SimMIM, iBOT, and SMIT, pretrained then fine-tuned on identical datasets for consistency. Standard confidence-based OOD methods (e.g., softmax scores) and conventional radiomics often fail under real-world distribution shifts and concept drift in medical images. Our feature-level OOD classifier, using a random forest trained with outlier exposure, outperforms the traditional approaches under diverse cases like pulmonary embolism, COVID-19, and unrelated organ scans. This work is tailored towards safer deployment of segmentation models in diverse and unpredictable clinical environments for diagnostic radiology.</p>
          </td>
          
        </tr></tbody></table>
      </td></tr>
      
      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2025_smart.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Self-distilled Masked Attention guided masked image modeling with noise Regularized Teacher (SMART) for medical image analysis
            </papertitle></a><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, Chloe Choi, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2310.01209">arXiv</a>
            <p style="text-align:justify;">SMART is a self-supervised framework designed for medical image analysis using Swin Transformers, which traditionally lack the <span style="background-color:rgb(239, 168, 168); font-style:italic;">[CLS]</span> token used in standard masked image modeling. We addressed this drawback by introducing a semantic attention module that performs global attention and identifies informative regions for masking during pretraining. To improve training stability and generalization, we incorporate a noise-regularized momentum teacher in a co-distillation setup. This approach yields a stronger foundation model, enabling more effective downstream tasks such as tumor and organ segmentation and zero-shot localization via attention maps with the <span style="background-color:rgb(239, 168, 168); font-style:italic;">[CLS]</span> token.

            </p>
          </td>
        </tr></tbody></table>
      </td></tr>
      
      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2025_heartsub.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers
            </papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>,  Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, <a href="https://www.mskcc.org/cancer-care/doctors/abraham-wu">Abraham Wu</a>, <a href="https://www.mskcc.org/profile/maria-thor">Maria Thor</a>, Andreas Rimner, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2505.10855">arXiv</a>
            <p style="text-align:justify;">We fine-tuned a pretrained transformer with a convolution network decoder for segmenting cardiac substructures from both contrast and non-contrast CT scans, with an emphasis on reducing the need for extensive annotated data. The transformer encoder was pretrained on in-the-wild large CT dataset and then adapted with fine-tuning for radiotherapy planning in lung cancer patients, with zero-shot application on breast cancer patients. Extensive experiments demonstrated strong generalization across imaging modalities, clinical sites, and patient positionings.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>
      
      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/paper2025_brat.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>brat: Aligned Multi-View Embeddings for Brain MRI Analysis
            </papertitle></a><br>
            <a href="https://www.bdi.ox.ac.uk/Team/maxime-kayser">Maxime Kayser</a>, Maksim Gridnev, Wanting Wang, Max Bain, <strong>Aneesh Rangnekar</strong>, Avijit Chatterjee, Aleksandr Petrov, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a>,<a href="https://www.mskcc.org/cancer-care/doctors/nathaniel-swinburne">Nathaniel C. Swinburne</a><br>
            <a href="papers/brat_overleaf_copy.pdf">Paper</a>
            <p style="text-align:justify;">We developed <span style="background-color:lightgreen; font-style:italic;">brat</span>
              (Brain Report Alignment Transformer), a new AI framework designed to align brain MRI scans with clinical radiology reports. We curated one of the largest dataset of its kind for this purpose - over 75,000 3D MRIs with paired radiologist reports. <span style="background-color:lightgreen; font-style:italic;">brat</span> learned rich, multi-view representations of the complex brain anatomy using self-supervised pretraining. We proposed a novel pairwise view alignment mechanism and a diversity-promoting loss based on Determinantal Point Processes. <span style="background-color:lightgreen; font-style:italic;">brat</span> significantly outperforms previous methods on tasks like image-text retrieval, tumor segmentation, and Alzheimer’s classification, and also enables high-quality automatic report generation from MRIs using language models

            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/arxiv2024_ovarian_aiguided.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Improving ovarian cancer segmentation accuracy with transformers through AI-guided labeling
            </papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/research-areas/labs/members/kevin-boehm">Kevin M. Boehm</a>, Emily A. Aherne, Ines Nikolovski, Natalie Gangai, Ying Liu, Dimitry Zamarin, Kara Roche, <a href="https://www.mskcc.org/research-areas/labs/sohrab-shah">Sohrab Shah</a>, <a href="https://www.mskcc.org/cancer-care/doctors/yulia-lakhman">Yulia Lakhman</a>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <a href="https://arxiv.org/pdf/2406.17666">arXiv</a>
            <p style="text-align:justify;">We formulated an AI-guided labeling pipeline using a multi-resolution residual 2D network trained on partially segmented CTs (adnexal tumors and omental implants) to assist radiologists in refining annotations. The enhanced dataset fine-tuned two transformer architectures, namely SMIT and Swin UNETR, and is evaluated across 71 multi-institutional 3D CT scans. Training with AI-refined labels yielded statistically significant improvements across all metrics for both models. Our approach emphasized efficient dataset curation, reducing annotation workload for radiologists while improving model performance and radiomics reproducibility in ovarian cancer.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/spie2025_quantuncertainty.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13409/1340914/Quantifying-uncertainty-in-lung-cancer-segmentation-with-foundation-models-applied/10.1117/12.3047709.short"><papertitle>Quantifying uncertainty in lung cancer segmentation with foundation models applied to mixed-domain datasets</papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, Nishant Nadkarni, <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em>SPIE Medical Imaging</em>, 2025<br>
            <a href="https://arxiv.org/pdf/2403.13113.pdf">arXiv</a>
            <p style="text-align:justify;">We investigated applications of foundation models on lung tumor segmentation across mixed-domain CT datasets, encompassing diverse acquisition protocols and institutions, as a stepping stone towards task generalization. Our study evaluated segmentation performance and uncertainty estimation using Monte Carlo dropout, deep ensembles, and test-time augmentation. We demonstrated that fast, entropy-based metrics and volumetric occupancy can effectively track model performance under domain shift, offering introductory practical tools to evaluate segmentation trustworthiness in mixed-domain clinical settings.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/medphycs2025_sslmodels.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.17541"><papertitle>Self‐supervised learning improves robustness of deep learning lung tumor segmentation models to CT imaging differences
            </papertitle></a><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em>Medical Physics</em>, 2025<br>
            <a href="https://arxiv.org/pdf/2405.08657">arXiv</a>
            <p style="text-align:justify;">We investigated the benefits of large-scale in-the-wild self-supervised pretraining on uncurated CT scans to improve robustness in tumor segmentatio, with focus on lung cancer tumors. When fine-tuned on smaller curated NSCLC datasets, Swin transformer models pretrained on this diverse unlabeled data, consistently outperformed both self-pretrained Swin and Vision Transformer counterparts across varied CT acquisition protocols. Masked image prediction proved more effective than contrastive learning at capturing local anatomical structure, enhancing accuracy and feature reuse. Our study directly addresses whether self-supervision on noisy, heterogeneous CT data improves generalization to real-world distribution shifts — a critical gap underaddressed by prior research.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/midl2025_codistilled.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=tLYEAmMupF#discussion"><papertitle>Co-distilled attention guided masked image modeling with noisy teacher for self-supervised learning on medical images
            </papertitle></a><br>
            <a href="https://www.mskcc.org/profile/jue-jiang">Jue Jiang</a>, <strong>Aneesh Rangnekar</strong>, <a href="https://www.mskcc.org/profile/harini-veeraraghavan">Harini Veeraraghavan</a><br>
            <em> Medical Imaging with Deep Learning</em>, 2025<br>
            <a href="papers/MIDL_2025__CoDistilled.pdf">Paper</a>
            <p style="text-align:justify;">We developed DAGMaN, a novel self-supervised learning framework that combined attention-guided masked image modeling and noisy teacher co-distillation for medical imaging. We integrated global semantic attention into Swin transformers with vision transformer based multi-head self-attention blocks and improved feature learning and attention diversity by noise injection in the teacher’s pipeline. Our approach outperformed prior methods on multiple medical tasks, including lung nodule classification, tumor segmentation, immunotherapy response prediction, and organ clustering. It also achieved high accuracy in few-shot settings, while enabling better interpretability via attention map visualization, a previously unavaible feature for Swin transformers.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/s4al.jpg' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Rangnekar_Semantic_Segmentation_With_Active_Semi-Supervised_Learning_WACV_2023_paper.pdf"><papertitle>Semantic Segmentation with Active Semi-Supervised Learning</papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>WACV</em>, 2023<br>
            <a href="https://arxiv.org/pdf/2203.10730">arXiv</a> / <a href="https://github.com/aneesh3108/S4AL">code</a>
            <p style="text-align:justify;">We developed S4AL, a hybrid approach that combined semi-supervised and active learning, for data-efficient semantic segmentation. It uses a teacher–student pseudo-labeling framework to generate region-level acquisition scores, enabling querying and annotating of only the most informative regions rather than full images. We introduced two regularization techniques, confidence weighting and balanced ClassMix, that helped mitigate class imbalance and enhance quality of the acquisition metric. S4AL achieved over 95% of full-dataset performance using less than 17% of pixel annotations on CamVid and CityScapes datasets.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/s4alplus.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://bmvc2022.mpi-inf.mpg.de/0229.pdf"><papertitle>Semantic Segmentation with Active Semi-Supervised Representation Learning</papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>BMVC</em>, 2022<br>
            <a href="https://arxiv.org/pdf/2210.08403">arXiv</a>
            <p style="text-align:justify;">We developed S4AL+, a hybrid framework that combines semi-supervised learning with active learning for semantic segmentation, aiming to reduce annotation costs. We replaced the conventional mean-teacher approach with self-training using noisy pseudo-labels and added a contrastive head for better feature learning of the classes. On benchmarks CamVid and CityScapes, S4AL+ achieved over 95% of full-label performance using just 12–15% of labeled data, outperforming state-of-the-art approaches at the time.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/specAL.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-52670-1_19"><papertitle>SpecAL: Towards Active Learning for Semantic Segmentation of Hyperspectral Imagery</papertitle></a><br>
            <strong>Aneesh Rangnekar</strong>, <a href="https://www.rit.edu/dirs/directory/ejipci-emmett-ientilucci">Emmett Ientilucci</a>, <a href="https://chriskanan.com/">Christopher Kanan</a>, <a href="https://people.rit.edu/mjhsma/">Matthew Hoffman</a><br>
            <em>DDDAS</em>, 2022<br>
            <a href="papers/DDDAS_2022__SpecAL.pdf">Paper</a>
            <p style="text-align:justify;">We proposed SpecAL, an active learning framework for semantic segmentation of hyperspectral imagery, reducing the need for extensive efforts for labeled data. Using the AeroRIT dataset, we combine data-efficient neural network design with self-supervised learning and batch-ensembles based uncertainty acquisition to iteratively improve performance. Our method design achieved oracle level segmentation performance using only 30% of the labeled data, demonstrating a scalable path for annotation-efficient hyperspectral analysis.
            </p>
          </td>
        </tr></tbody></table>
      </td></tr>

      <!-- Paper -->
      <tr><td>
        <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody><tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/tgrs2020_aerorit.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/abstract/document/9079478/"><papertitle>AeroRIT: A New Scene for Hyperspectral Image Analysis</papertitle></a><br>
            <strong>Aneesh Rangnekar</strong> et al.<br>
            <em>IEEE TGRS</em>, 2020<br>
            <a href="https://arxiv.org/pdf/1912.08178.pdf">arXiv</a> / <a href="https://github.com/aneesh3108/AeroRIT">code</a>
            <p style="text-align:justify;">We introduced AeroRIT, a new aerial hyperspectral dataset designed specifically to support convolutional neural network (CNN) training for scene understanding. Unlike typical airborne hyperspectral datasets focused on vegetation or roads, AeroRIT includes buildings and cars, expanding the domain diversity. We design and benchmark several CNN architectures on AeroRIT, thoroughly evaluating classification accuracy, spatial consistency, and generalization across scenes. </p>
          </td>
        </tr></tbody></table>
      </td></tr>


      <!-- Footer -->
      <table style="width:100%;border:0;border-spacing:0;margin:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br><p style="text-align:center;font-size:small;">
              This website is adapted from Jon Barron's template (<a href="https://jonbarron.info/">site</a>, <a href="https://github.com/jonbarron/jonbarron_website">source code</a>)
            </p>
          </td>
        </tr>
      </tbody></table>

    </td></tr>
  </tbody></table>
</body>
</html>
